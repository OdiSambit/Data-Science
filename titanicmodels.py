# -*- coding: utf-8 -*-
"""TitanicModels.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p5yQcij_Gi4854R1mONX9ZQJksuDNA1V
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df=sns.load_dataset('titanic')
df.head()

df.columns

df.info()
#to check how many null values

"""We have droped the values which had max null values

"""

df.drop(['deck','embark_town','alive','class','who','adult_male'],axis=1,inplace=True)

df.info()

"""We have filled all the null values with mean"""

df['age'].fillna(df['age'].mean(),inplace=True)
df.info()

"""So here we have 891 entries except embarked so we will drop two rows from every col to make it 889"""

df.dropna(subset=['embarked'],inplace=True)
df.info()

#so we have 2 cols which isin object so we need label encoding

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
df['sex']=le.fit_transform(df['sex'])#0,1
df['embarked']=le.fit_transform(df['embarked'])#s=2,C=0,Q=1
df.head()

x=df.drop('survived',axis=1)#input
y=df['survived']#output

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

from sklearn.linear_model import LogisticRegression
model=LogisticRegression()
model.fit(x_train,y_train)

y_pred= model.predict(x_test)
y_pred

"""This is confusion martrix. So in logistic regression model to see if your model is working or not to see that we use logistic regression"""

y_test

from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
accuracy_score(y_test,y_pred)

confusion_matrix(y_test,y_pred)

print(classification_report(y_test,y_pred))

"""method 2"""

#KNN
#Feature Scalling
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train_scaled=sc.fit_transform(x_train)
x_test_scaled=sc.fit_transform(x_test)

from sklearn.neighbors import KNeighborsClassifier
knn_model=KNeighborsClassifier(n_neighbors=5)
knn_model.fit(x_train_scaled,y_train)

y_pred_knn =knn_model.predict(x_test_scaled)

accuracy_score(y_test,y_pred_knn)

confusion_matrix(y_test,y_pred_knn)

print(classification_report(y_test,y_pred_knn))

"""method 3"""

#Naive Bayes
from sklearn.naive_bayes import GaussianNB
nb_model=GaussianNB()
nb_model.fit(x_train,y_train)

y_pred_nb=nb_model.predict(x_test)
y_pred_nb

accuracy_score(y_test,y_pred_nb)

confusion_matrix(y_test,y_pred_nb)

print(classification_report(y_test,y_pred_nb))

"""method 4"""

#decision tree
#feature scalling required
from sklearn.tree import DecisionTreeClassifier
dt_model=DecisionTreeClassifier(random_state=42)
dt_model.fit(x_train_scaled,y_train)

y_pred_dt = dt_model.predict(x_test_scaled)
y_pred_dt

accuracy_score(y_test,y_pred_dt)

confusion_matrix(y_test,y_pred_dt)

print(classification_report(y_test,y_pred_dt))

"""for continuous values like age ,fare information gain is calculated using mean squared error

Method 5
"""

#support vector machine
from sklearn.svm import SVC
model_svm = SVC(kernel='rbf')#rbf kernel is widely used kernel
model_svm.fit(x_train_scaled,y_train)

y_pred_svc =model_svm.predict(x_test_scaled)
y_pred_svc

accuracy_score(y_test,y_pred_svc)

#till now the model which performed quite well and more than accuracy predicated by logistic regression
confusion_matrix(y_test,y_pred_svc)

print(classification_report(y_test,y_pred_svc))

